{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import csv\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertModel\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List, Dict\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import List\n",
    "import tqdm\n",
    "import csv\n",
    "import json\n",
    "import faiss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"results.covid_dataset.all.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, sents = df[\"sentence_id\"].tolist(), df[\"sentence_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2318939"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sequences in bold indicate codons requiring multiple nucleotide substitutions .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'These analytic weights were then multiplied by the appropriate adjustment factors as described above .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(object):\n",
    "    \n",
    "    def __init__(self, device = 'cpu'):\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('/media/shauli/Elements/current_projects/AI2/Clustering/scibert_scivocab_uncased/vocab.txt')\n",
    "        self.model = BertModel.from_pretrained('/media/shauli/Elements/current_projects/AI2/Clustering/scibert_scivocab_uncased/')\n",
    "            \n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "        self.device = device\n",
    "        self.pad_token = self.tokenizer.convert_tokens_to_ids([self.tokenizer.pad_token])[0]\n",
    "        \n",
    "    def tokenize_and_pad(self, texts: List[str]):\n",
    "        \n",
    "        indexed_texts = [self.tokenizer.encode(text, add_special_tokens=True, max_length = 512) for text in texts] #\n",
    "        max_len = max(len(text) for text in indexed_texts)\n",
    "        indexed_texts = [text + [self.pad_token] * (max_len - len(text)) for text in indexed_texts]\n",
    "        idx_tensor = torch.LongTensor(indexed_texts).to(self.device)\n",
    "        att_tensor = idx_tensor != self.pad_token\n",
    "        \n",
    "        return idx_tensor, att_tensor\n",
    "    \n",
    "    def encode(self, sentences: List[str], sentence_ids: List[str], batch_size: int, strategy: str = \"cls\"):\n",
    "        assert len(sentences) == len(sentence_ids)\n",
    "        \n",
    "        with open(\"output.jsonl\", \"w\", encoding = \"utf-8\") as f:\n",
    "            \n",
    "            for batch_idx in tqdm.tqdm_notebook(range(0, len(sentences), batch_size), total = len(sentences)//batch_size):\n",
    "            \n",
    "                batch_sents = sentences[batch_idx: batch_idx + batch_size]\n",
    "                batch_ids = sentence_ids[batch_idx: batch_idx + batch_size]\n",
    "                assert len(batch_sents) == len(batch_ids)\n",
    "                \n",
    "                idx, att = self.tokenize_and_pad(batch_sents)\n",
    "            \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(idx)\n",
    "                    last_hidden = outputs[0]\n",
    "                \n",
    "                    if strategy == \"cls\":\n",
    "                        h = last_hidden[:, 0, ...]\n",
    "                    elif strategy == \"mean\":\n",
    "                        h = torch.mean(last_hidden, axis = 1)\n",
    "                    elif strategy == \"median\":\n",
    "                        h = torch.median(last_hidden, axis = 1).values\n",
    "                    elif strategy == \"max\":\n",
    "                        h = torch.max(last_hidden, axis = 1).values\n",
    "                    elif strategy == \"min\":\n",
    "                        h = torch.min(last_hidden, axis = 1).values\n",
    "            \n",
    "                batch_np = h.detach().cpu().numpy()\n",
    "                assert len(batch_np) == len(batch_sents)\n",
    "                \n",
    "                sents_states_ids = zip(batch_sents, batch_np, batch_ids)\n",
    "                for sent, vec, sent_id in sents_states_ids:\n",
    "                    \n",
    "                    vec_str = \" \".join([\"%.4f\" % x for x in vec])\n",
    "                    sent_dict = {\"text\": sent, \"vec\": vec_str, \"id\": sent_id}\n",
    "                    f.write(json.dumps(sent_dict) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "encoder = BertEncoder(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11beb2f44fc4953b7995bbb6c7fc2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=72466), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "h = encoder.encode(sents, ids, batch_size = 32, strategy = \"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50000\n",
    "data = []\n",
    "\n",
    "with open(\"output.jsonl\", \"r\", encoding = \"utf-8\") as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "            data_dict = eval(line)\n",
    "            data_dict[\"vec\"] = np.array([float(x) for x in data_dict[\"vec\"].split(\" \")]).astype(\"float32\")\n",
    "            data.append(data_dict)\n",
    "            i += 1\n",
    "            if i > n: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_sample = np.array([d[\"vec\"] for d in data])\n",
    "sents_sample =[d[\"text\"] for d in data]\n",
    "pca = PCA(n_components = 0.98)\n",
    "vecs_sample_pca = pca.fit_transform(vecs_sample).astype(\"float32\")\n",
    "\n",
    "index = faiss.IndexFlatIP(vecs_sample_pca.shape[1])\n",
    "index.add(np.ascontiguousarray(vecs_sample_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "spread_idx = np.array([i for i,sent in enumerate(sents_sample) if \"spread\" in sent])\n",
    "spread_mean = np.median(vecs_sample[spread_idx], axis = 0)\n",
    "spread_mean_pca = pca.transform([spread_mean])\n",
    "D, I = index.search(np.ascontiguousarray(spread_mean_pca), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 27,  29,  37,  38,  87,  91,  94, 146, 185, 230])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spread_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'These findings highlight a new explanatory pattern for the risk of HPAI and indicate that , in addition to agro-environmental factors , anthropogenic factors play an important role in the spread of H5N1 .'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parasitism of vegetal organisms also had a great impact in the history of the humanity .\n",
      "===========================================================\n",
      "None of the infected animals became ill , despite the reported virulence of the challenge viruses .\n",
      "===========================================================\n",
      "The olfactory pathway is amenable to surgical intervention at the level of the nasal epithelium , the olfactory nerve , or the olfactory bulb .\n",
      "===========================================================\n",
      "New targets for novel anti-infective drugs need to be identified , particularly in the light of the emergence of drug resistant strains .\n",
      "===========================================================\n",
      "Indeed , the CHWs stressed the importance of the various incentives in carrying out their duties .\n",
      "===========================================================\n",
      "HPNV-specific transcripts were detected in the cells in the intestine , indicating that this virus replicates in the intestinal tract .\n",
      "===========================================================\n",
      "Interestingly , the procedure involved the delivery of the biological preparation through intranasal inoculation , a common route of modern mucosal vaccination .\n",
      "===========================================================\n",
      "this study evaluated the role of pmn during a non-fatal pulmonary coronavirus infection in the natural host .\n",
      "===========================================================\n",
      "This would allow the investigation of the relative contribution of pneumococcal infection in causing pneumonia , as well as the role of other pathogens .\n",
      "===========================================================\n"
     ]
    }
   ],
   "source": [
    "for i in I.squeeze()[1:]:\n",
    "    print(sents_sample[i])\n",
    "    print(\"===========================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Poisson regression with analytic weights , offsets , and robust variance estimation was used to implement the above extrapolation and standardization procedures for estimating seasonal incidence and 95 % confidence intervals ( CIs ) [ 17 ] [ 18 ] [ 19 ] .'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
